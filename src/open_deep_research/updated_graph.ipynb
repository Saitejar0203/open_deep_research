{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install required packages\n!pip install -q langgraph langchain-community langchain-anthropic tavily-python",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Set API keys here\nimport os, sys\nsys.path.append('..')  # Adjust path if needed\nos.environ['TAVILY_API_KEY'] = 'your-tavily-api-key'\nos.environ['ANTHROPIC_API_KEY'] = 'your-anthropic-api-key'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from typing import Literal\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.runnables import RunnableConfig\n\nfrom langgraph.constants import Send\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.types import interrupt, Command\n\nfrom open_deep_research.state import (\n    ReportStateInput,\n    ReportStateOutput,\n    Sections,\n    ReportState,\n    SectionState,\n    SectionOutputState,\n    Queries,\n    Feedback\n)\n\nfrom open_deep_research.prompts import (\n    report_planner_query_writer_instructions,\n    report_planner_instructions,\n    query_writer_instructions, \n    section_writer_instructions,\n    final_section_writer_instructions,\n    section_grader_instructions,\n    section_writer_inputs\n)\n\nfrom open_deep_research.configuration import WorkflowConfiguration\nfrom open_deep_research.utils import (\n    format_sections, \n    get_config_value, \n    get_search_params, \n    select_and_execute_search,\n    get_today_str\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "async def generate_report_plan(state: ReportState, config: RunnableConfig):\n    \"\"\"Generate the initial report plan with sections.\n    \n    This node:\n    1. Gets configuration for the report structure and search parameters\n    2. Generates search queries to gather context for planning\n    3. Performs web searches using those queries\n    4. Uses an LLM to generate a structured plan with sections\n    \n    Args:\n        state: Current graph state containing the report topic\n        config: Configuration for models, search APIs, etc.\n        \n    Returns:\n        Dict containing the generated sections\n    \"\"\"\n\n    # Inputs\n    topic = state[\"topic\"]\n\n    # Get list of feedback on the report plan\n    feedback_list = state.get(\"feedback_on_report_plan\", [])\n\n    # Concatenate feedback on the report plan into a single string\n    feedback = \" /// \".join(feedback_list) if feedback_list else \"\"\n\n    # Get configuration\n    configurable = WorkflowConfiguration.from_runnable_config(config)\n    report_structure = configurable.report_structure\n    number_of_queries = configurable.number_of_queries\n    search_api = get_config_value(configurable.search_api)\n    search_api_config = configurable.search_api_config or {}  # Get the config dict, default to empty\n    params_to_pass = get_search_params(search_api, search_api_config)  # Filter parameters\n\n    # Convert JSON object to string if necessary\n    if isinstance(report_structure, dict):\n        report_structure = str(report_structure)\n\n    # Set writer model (model used for query writing)\n    writer_provider = get_config_value(configurable.writer_provider)\n    writer_model_name = get_config_value(configurable.writer_model)\n    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})\n    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) \n    structured_llm = writer_model.with_structured_output(Queries)\n\n    # Format system instructions\n    system_instructions_query = report_planner_query_writer_instructions.format(\n        topic=topic,\n        report_organization=report_structure,\n        number_of_queries=number_of_queries,\n        today=get_today_str()\n    )\n\n    # Generate queries  \n    results = await structured_llm.ainvoke([SystemMessage(content=system_instructions_query),\n                                     HumanMessage(content=\"Generate search queries that will help with planning the sections of the report.\")])\n\n    # Web search\n    query_list = [query.search_query for query in results.queries]\n\n    # Search the web with parameters\n    source_str = await select_and_execute_search(search_api, query_list, params_to_pass)\n\n    # Format system instructions\n    system_instructions_sections = report_planner_instructions.format(topic=topic, report_organization=report_structure, context=source_str, feedback=feedback)\n\n    # Set the planner\n    planner_provider = get_config_value(configurable.planner_provider)\n    planner_model = get_config_value(configurable.planner_model)\n    planner_model_kwargs = get_config_value(configurable.planner_model_kwargs or {})\n\n    # Report planner instructions\n    planner_message = \"\"\"Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. \n                        Each section must have: name, description, research, and content fields.\"\"\"\n\n    # Run the planner\n    if planner_model == \"claude-3-7-sonnet-latest\":\n        # Allocate a thinking budget for claude-3-7-sonnet-latest as the planner model\n        planner_llm = init_chat_model(model=planner_model, \n                                      model_provider=planner_provider, \n                                      max_tokens=20_000, \n                                      thinking={\"type\": \"enabled\", \"budget_tokens\": 16_000})\n\n    else:\n        # With other models, thinking tokens are not specifically allocated\n        planner_llm = init_chat_model(model=planner_model, \n                                      model_provider=planner_provider,\n                                      model_kwargs=planner_model_kwargs)\n    \n    # Generate the report sections\n    structured_llm = planner_llm.with_structured_output(Sections)\n    report_sections = await structured_llm.ainvoke([SystemMessage(content=system_instructions_sections),\n                                             HumanMessage(content=planner_message)])\n\n    # Get sections\n    sections = report_sections.sections\n\n    return {\"sections\": sections}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def human_feedback(state: ReportState, config: RunnableConfig) -> Command[Literal[\"generate_report_plan\",\"build_section_with_web_research\"]]:\n    \"\"\"Get human feedback on the report plan and route to next steps.\n    \n    This node:\n    1. Formats the current report plan for human review\n    2. Gets feedback via an interrupt\n    3. Routes to either:\n       - Section writing if plan is approved\n       - Plan regeneration if feedback is provided\n    \n    Args:\n        state: Current graph state with sections to review\n        config: Configuration for the workflow\n        \n    Returns:\n        Command to either regenerate plan or start section writing\n    \"\"\"\n\n    # Get sections\n    topic = state[\"topic\"]\n    sections = state['sections']\n    sections_str = \"\\n\\n\".join(\n        f\"Section: {section.name}\\n\"\n        f\"Description: {section.description}\\n\"\n        f\"Research needed: {'Yes' if section.research else 'No'}\\n\"\n        for section in sections\n    )\n\n    # Get feedback on the report plan from interrupt\n    interrupt_message = f\"\"\"Please provide feedback on the following report plan. \n                        \\n\\n{sections_str}\\n\n                        \\nDoes the report plan meet your needs?\\nPass 'true' to approve the report plan.\\nOr, provide feedback to regenerate the report plan:\"\"\"\n    \n    feedback = interrupt(interrupt_message)\n\n    # If the user approves the report plan, kick off section writing\n    if isinstance(feedback, bool) and feedback is True:\n        # Treat this as approve and kick off section writing\n        return Command(goto=[\n            Send(\"build_section_with_web_research\", {\"topic\": topic, \"section\": s, \"search_iterations\": 0}) \n            for s in sections \n            if s.research\n        ])\n    \n    # If the user provides feedback, regenerate the report plan \n    elif isinstance(feedback, str):\n        # Treat this as feedback and append it to the existing list\n        return Command(goto=\"generate_report_plan\", \n                       update={\"feedback_on_report_plan\": [feedback]})\n    else:\n        raise TypeError(f\"Interrupt value of type {type(feedback)} is not supported.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "async def generate_queries(state: SectionState, config: RunnableConfig):\n    \"\"\"Generate search queries for researching a specific section.\n    \n    This node uses an LLM to generate targeted search queries based on the \n    section topic and description.\n    \n    Args:\n        state: Current state containing section details\n        config: Configuration including number of queries to generate\n        \n    Returns:\n        Dict containing the generated search queries\n    \"\"\"\n\n    # Get state \n    topic = state[\"topic\"]\n    section = state[\"section\"]\n\n    # Get configuration\n    configurable = WorkflowConfiguration.from_runnable_config(config)\n    number_of_queries = configurable.number_of_queries\n\n    # Generate queries \n    writer_provider = get_config_value(configurable.writer_provider)\n    writer_model_name = get_config_value(configurable.writer_model)\n    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})\n    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) \n    structured_llm = writer_model.with_structured_output(Queries)\n\n    # Format system instructions\n    system_instructions = query_writer_instructions.format(topic=topic, \n                                                           section_topic=section.description, \n                                                           number_of_queries=number_of_queries,\n                                                           today=get_today_str())\n\n    # Generate queries  \n    queries = await structured_llm.ainvoke([SystemMessage(content=system_instructions),\n                                     HumanMessage(content=\"Generate search queries on the provided topic.\")])\n\n    return {\"search_queries\": queries.queries}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "async def search_web(state: SectionState, config: RunnableConfig):\n    \"\"\"Execute web searches for the section queries.\n    \n    This node:\n    1. Takes the generated queries\n    2. Executes searches using configured search API\n    3. Formats results into usable context\n    \n    Args:\n        state: Current state with search queries\n        config: Search API configuration\n        \n    Returns:\n        Dict with search results and updated iteration count\n    \"\"\"\n\n    # Get state\n    search_queries = state[\"search_queries\"]\n\n    # Get configuration\n    configurable = WorkflowConfiguration.from_runnable_config(config)\n    search_api = get_config_value(configurable.search_api)\n    search_api_config = configurable.search_api_config or {}  # Get the config dict, default to empty\n    params_to_pass = get_search_params(search_api, search_api_config)  # Filter parameters\n\n    # Web search\n    query_list = [query.search_query for query in search_queries]\n\n    # Search the web with parameters\n    source_str = await select_and_execute_search(search_api, query_list, params_to_pass)\n\n    return {\"source_str\": source_str, \"search_iterations\": state[\"search_iterations\"] + 1}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "async def write_section(state: SectionState, config: RunnableConfig) -> Command[Literal[END, \"search_web\"]]:\n    \"\"\"Write a section of the report and evaluate if more research is needed.\n    \n    This node:\n    1. Writes section content using search results\n    2. Evaluates the quality of the section\n    3. Either:\n       - Completes the section if quality passes\n       - Triggers more research if quality fails\n    \n    Args:\n        state: Current state with search results and section info\n        config: Configuration for writing and evaluation\n        \n    Returns:\n        Command to either complete section or do more research\n    \"\"\"\n\n    # Get state \n    topic = state[\"topic\"]\n    section = state[\"section\"]\n    source_str = state[\"source_str\"]\n\n    # Get configuration\n    configurable = WorkflowConfiguration.from_runnable_config(config)\n\n    # Format system instructions\n    section_writer_inputs_formatted = section_writer_inputs.format(topic=topic, \n                                                             section_name=section.name, \n                                                             section_topic=section.description, \n                                                             context=source_str, \n                                                             section_content=section.content)\n\n    # Generate section  \n    writer_provider = get_config_value(configurable.writer_provider)\n    writer_model_name = get_config_value(configurable.writer_model)\n    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})\n    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) \n\n    section_content = await writer_model.ainvoke([SystemMessage(content=section_writer_instructions),\n                                           HumanMessage(content=section_writer_inputs_formatted)])\n    \n    # Write content to the section object  \n    section.content = section_content.content\n\n    # Grade prompt \n    section_grader_message = (\"Grade the report and consider follow-up questions for missing information. \"\n                              \"If the grade is 'pass', return empty strings for all follow-up queries. \"\n                              \"If the grade is 'fail', provide specific search queries to gather missing information.\")\n    \n    section_grader_instructions_formatted = section_grader_instructions.format(topic=topic, \n                                                                               section_topic=section.description,\n                                                                               section=section.content, \n                                                                               number_of_follow_up_queries=configurable.number_of_queries)\n\n    # Use planner model for reflection\n    planner_provider = get_config_value(configurable.planner_provider)\n    planner_model = get_config_value(configurable.planner_model)\n    planner_model_kwargs = get_config_value(configurable.planner_model_kwargs or {})\n\n    if planner_model == \"claude-3-7-sonnet-latest\":\n        # Allocate a thinking budget for claude-3-7-sonnet-latest as the planner model\n        reflection_model = init_chat_model(model=planner_model, \n                                           model_provider=planner_provider, \n                                           max_tokens=20_000, \n                                           thinking={\"type\": \"enabled\", \"budget_tokens\": 16_000}).with_structured_output(Feedback)\n    else:\n        reflection_model = init_chat_model(model=planner_model, \n                                           model_provider=planner_provider, model_kwargs=planner_model_kwargs).with_structured_output(Feedback)\n    # Generate feedback\n    feedback = await reflection_model.ainvoke([SystemMessage(content=section_grader_instructions_formatted),\n                                        HumanMessage(content=section_grader_message)])\n\n    # If the section is passing or the max search depth is reached, publish the section to completed sections \n    if feedback.grade == \"pass\" or state[\"search_iterations\"] >= configurable.max_search_depth:\n        # Publish the section to completed sections \n        update = {\"completed_sections\": [section]}\n        if configurable.include_source_str:\n            update[\"source_str\"] = source_str\n        return Command(update=update, goto=END)\n\n    # Update the existing section with new content and update search queries\n    else:\n        return Command(\n            update={\"search_queries\": feedback.follow_up_queries, \"section\": section},\n            goto=\"search_web\"\n        )",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "async def write_final_sections(state: SectionState, config: RunnableConfig):\n    \"\"\"Write sections that don't require research using completed sections as context.\n    \n    This node handles sections like conclusions or summaries that build on\n    the researched sections rather than requiring direct research.\n    \n    Args:\n        state: Current state with completed sections as context\n        config: Configuration for the writing model\n        \n    Returns:\n        Dict containing the newly written section\n    \"\"\"\n\n    # Get configuration\n    configurable = WorkflowConfiguration.from_runnable_config(config)\n\n    # Get state \n    topic = state[\"topic\"]\n    section = state[\"section\"]\n    completed_report_sections = state[\"report_sections_from_research\"]\n    \n    # Format system instructions\n    system_instructions = final_section_writer_instructions.format(topic=topic, section_name=section.name, section_topic=section.description, context=completed_report_sections)\n\n    # Generate section  \n    writer_provider = get_config_value(configurable.writer_provider)\n    writer_model_name = get_config_value(configurable.writer_model)\n    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})\n    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) \n    \n    section_content = await writer_model.ainvoke([SystemMessage(content=system_instructions),\n                                           HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n    \n    # Write content to section \n    section.content = section_content.content\n\n    # Write the updated section to completed sections\n    return {\"completed_sections\": [section]}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def gather_completed_sections(state: ReportState):\n    \"\"\"Format completed sections as context for writing final sections.\n    \n    This node takes all completed research sections and formats them into\n    a single context string for writing summary sections.\n    \n    Args:\n        state: Current state with completed sections\n        \n    Returns:\n        Dict with formatted sections as context\n    \"\"\"\n\n    # List of completed sections\n    completed_sections = state[\"completed_sections\"]\n\n    # Format completed section to str to use as context for final sections\n    completed_report_sections = format_sections(completed_sections)\n\n    return {\"report_sections_from_research\": completed_report_sections}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def compile_final_report(state: ReportState, config: RunnableConfig):\n    \"\"\"Compile all sections into the final report.\n    \n    This node:\n    1. Gets all completed sections\n    2. Orders them according to original plan\n    3. Combines them into the final report\n    \n    Args:\n        state: Current state with all completed sections\n        \n    Returns:\n        Dict containing the complete report\n    \"\"\"\n\n    # Get configuration\n    configurable = WorkflowConfiguration.from_runnable_config(config)\n\n    # Get sections\n    sections = state[\"sections\"]\n    completed_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n\n    # Update sections with completed content while maintaining original order\n    for section in sections:\n        section.content = completed_sections[section.name]\n\n    # Compile final report\n    all_sections = \"\\n\\n\".join([s.content for s in sections])\n\n    if configurable.include_source_str:\n        return {\"final_report\": all_sections, \"source_str\": state[\"source_str\"]}\n    else:\n        return {\"final_report\": all_sections}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def initiate_final_section_writing(state: ReportState):\n    \"\"\"Create parallel tasks for writing non-research sections.\n    \n    This edge function identifies sections that don't need research and\n    creates parallel writing tasks for each one.\n    \n    Args:\n        state: Current state with all sections and research context\n        \n    Returns:\n        List of Send commands for parallel section writing\n    \"\"\"\n\n    # Kick off section writing in parallel via Send() API for any sections that do not require research\n    return [\n        Send(\"write_final_sections\", {\"topic\": state[\"topic\"], \"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]}) \n        for s in state[\"sections\"] \n        if not s.research\n    ]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Report section sub-graph -- \n\n# Add nodes \nsection_builder = StateGraph(SectionState, output=SectionOutputState)\nsection_builder.add_node(\"generate_queries\", generate_queries)\nsection_builder.add_node(\"search_web\", search_web)\nsection_builder.add_node(\"write_section\", write_section)\n\n# Add edges\nsection_builder.add_edge(START, \"generate_queries\")\nsection_builder.add_edge(\"generate_queries\", \"search_web\")\nsection_builder.add_edge(\"search_web\", \"write_section\")\n\n# Outer graph for initial report plan compiling results from each section -- \n\n# Add nodes\nbuilder = StateGraph(ReportState, input=ReportStateInput, output=ReportStateOutput, config_schema=WorkflowConfiguration)\nbuilder.add_node(\"generate_report_plan\", generate_report_plan)\nbuilder.add_node(\"human_feedback\", human_feedback)\nbuilder.add_node(\"build_section_with_web_research\", section_builder.compile())\nbuilder.add_node(\"gather_completed_sections\", gather_completed_sections)\nbuilder.add_node(\"write_final_sections\", write_final_sections)\nbuilder.add_node(\"compile_final_report\", compile_final_report)\n\n# Add edges\nbuilder.add_edge(START, \"generate_report_plan\")\nbuilder.add_edge(\"generate_report_plan\", \"human_feedback\")\nbuilder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\nbuilder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\nbuilder.add_edge(\"write_final_sections\", \"compile_final_report\")\nbuilder.add_edge(\"compile_final_report\", END)\n\ngraph = builder.compile()",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
